{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwAVqPRg/7adxtNbBnD1TH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SidU/LLMs-from-scratch/blob/main/BytePairEncoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding Byte Pair Encoding (BPE)\n",
        "\n",
        "### Introduction\n",
        "Byte Pair Encoding (BPE) is a popular tokenization algorithm used in Natural Language Processing (NLP). It helps break down text into subword units, making it easier for models to handle rare words and unknown tokens efficiently.\n",
        "\n",
        "### Objectives\n",
        "- Understand how BPE works step by step.\n",
        "- Implement BPE using Python.\n",
        "- Visualize the process with real examples.\n",
        "\n",
        "This notebook will guide you through the algorithm and its application.\n"
      ],
      "metadata": {
        "id": "i39E18pE27IQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "qQ9lqg-e24LE"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import collections\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Initial Dataset\n",
        "To demonstrate BPE, we'll use a small dataset of words. These words will be split into characters for initial processing.\n"
      ],
      "metadata": {
        "id": "2CggsckI3Ds9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample dataset\n",
        "corpus = [\n",
        "    \"low\",\n",
        "    \"lowest\",\n",
        "    \"new\",\n",
        "    \"wider\"\n",
        "]\n",
        "\n",
        "# Display the dataset\n",
        "print(\"Initial corpus:\", corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jgJsfl93EuU",
        "outputId": "be96edb4-61e9-41fc-9837-03e40d1efaea"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial corpus: ['low', 'lowest', 'new', 'wider']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Tokenization at Character Level\n",
        "We start by splitting each word in the dataset into characters. We'll also add a special marker `</w>` to indicate the end of a word.\n"
      ],
      "metadata": {
        "id": "1lGmcrel3J1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to tokenize each word into characters\n",
        "def tokenize_corpus(corpus):\n",
        "    return [\" \".join(word) + \" </w>\" for word in corpus]  # Add </w> to mark end of word\n",
        "\n",
        "# Tokenize the corpus\n",
        "tokenized_corpus = tokenize_corpus(corpus)\n",
        "\n",
        "# Display tokenized words\n",
        "print(\"Tokenized corpus:\", tokenized_corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_8Bc_Am3Op_",
        "outputId": "6226e91f-49ae-401f-e94a-496592f7bbba"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized corpus: ['l o w </w>', 'l o w e s t </w>', 'n e w </w>', 'w i d e r </w>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Count Pair Frequencies\n",
        "Next, we'll count the frequency of consecutive pairs of tokens (characters or subwords) across the entire dataset."
      ],
      "metadata": {
        "id": "MoLP7LrI3XB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to count consecutive token pair frequencies\n",
        "def get_pair_frequencies(corpus):\n",
        "    pairs = collections.defaultdict(int)\n",
        "    for word in corpus:\n",
        "        symbols = word.split()\n",
        "        for i in range(len(symbols) - 1):\n",
        "            pairs[(symbols[i], symbols[i + 1])] += 1\n",
        "    return pairs\n",
        "\n",
        "# Count pair frequencies\n",
        "pair_frequencies = get_pair_frequencies(tokenized_corpus)\n",
        "\n",
        "# Display pair frequencies\n",
        "print(\"\\n\".join(f\"{pair}: {frequency}\" for pair, frequency in pair_frequencies.items()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8ZhFpFA3ZOR",
        "outputId": "c5e7c8dc-d3eb-4efa-845c-ec6649fbdf02"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('l', 'o'): 2\n",
            "('o', 'w'): 2\n",
            "('w', '</w>'): 2\n",
            "('w', 'e'): 1\n",
            "('e', 's'): 1\n",
            "('s', 't'): 1\n",
            "('t', '</w>'): 1\n",
            "('n', 'e'): 1\n",
            "('e', 'w'): 1\n",
            "('w', 'i'): 1\n",
            "('i', 'd'): 1\n",
            "('d', 'e'): 1\n",
            "('e', 'r'): 1\n",
            "('r', '</w>'): 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Merge the Most Frequent Pair\n",
        "The most frequent pair of tokens will be merged into a single token. This process reduces the number of tokens and captures frequent patterns."
      ],
      "metadata": {
        "id": "dxyBhHmY3dxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to merge the most frequent pair in the corpus\n",
        "def merge_pair(pair, corpus):\n",
        "    merged_corpus = []\n",
        "    bigram = re.escape(\" \".join(pair))\n",
        "    pattern = re.compile(rf\"(?<!\\S){bigram}(?!\\S)\")\n",
        "    for word in corpus:\n",
        "        merged_word = pattern.sub(\"\".join(pair), word)\n",
        "        merged_corpus.append(merged_word)\n",
        "    return merged_corpus\n",
        "\n",
        "# Identify the most frequent pair\n",
        "most_frequent_pair = max(pair_frequencies, key=pair_frequencies.get)\n",
        "\n",
        "# Merge the most frequent pair\n",
        "tokenized_corpus = merge_pair(most_frequent_pair, tokenized_corpus)\n",
        "\n",
        "# Display updated corpus after merging\n",
        "print(\"Updated corpus after merging:\", tokenized_corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AL3Kr2h3f3y",
        "outputId": "7d6195f4-ecb1-445c-fad5-e05104cb4845"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated corpus after merging: ['lo w </w>', 'lo w e s t </w>', 'n e w </w>', 'w i d e r </w>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Iterative Merging\n",
        "We repeat the process of counting pair frequencies and merging the most frequent pair until the desired vocabulary size is reached."
      ],
      "metadata": {
        "id": "5mRs0dQT3lYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform BPE for a fixed number of iterations\n",
        "def perform_bpe(corpus, num_merges):\n",
        "\n",
        "    for _ in range(num_merges):\n",
        "\n",
        "        # Step 1: Count pair frequencies\n",
        "        pairs = get_pair_frequencies(corpus)\n",
        "        if not pairs:\n",
        "            break\n",
        "\n",
        "        # Step 2: Find the most frequent pair\n",
        "        most_frequent_pair = max(pairs, key=pairs.get)\n",
        "\n",
        "        # Step 3: Merge the most frequent pair\n",
        "        corpus = merge_pair(most_frequent_pair, corpus)\n",
        "\n",
        "        # Display progress\n",
        "        print(f\"Most frequent pair: {most_frequent_pair}\")\n",
        "        print(\"Updated corpus:\", corpus)\n",
        "\n",
        "    return corpus\n",
        "\n",
        "# Run BPE with 10 merges\n",
        "final_corpus = perform_bpe(tokenized_corpus, num_merges=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6BSbd0W3ncL",
        "outputId": "68f37aae-c22e-4930-f561-fad89f358c9f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most frequent pair: ('lo', 'w')\n",
            "Updated corpus: ['low </w>', 'low e s t </w>', 'n e w </w>', 'w i d e r </w>']\n",
            "Most frequent pair: ('low', '</w>')\n",
            "Updated corpus: ['low</w>', 'low e s t </w>', 'n e w </w>', 'w i d e r </w>']\n",
            "Most frequent pair: ('low', 'e')\n",
            "Updated corpus: ['low</w>', 'lowe s t </w>', 'n e w </w>', 'w i d e r </w>']\n",
            "Most frequent pair: ('lowe', 's')\n",
            "Updated corpus: ['low</w>', 'lowes t </w>', 'n e w </w>', 'w i d e r </w>']\n",
            "Most frequent pair: ('lowes', 't')\n",
            "Updated corpus: ['low</w>', 'lowest </w>', 'n e w </w>', 'w i d e r </w>']\n",
            "Most frequent pair: ('lowest', '</w>')\n",
            "Updated corpus: ['low</w>', 'lowest</w>', 'n e w </w>', 'w i d e r </w>']\n",
            "Most frequent pair: ('n', 'e')\n",
            "Updated corpus: ['low</w>', 'lowest</w>', 'ne w </w>', 'w i d e r </w>']\n",
            "Most frequent pair: ('ne', 'w')\n",
            "Updated corpus: ['low</w>', 'lowest</w>', 'new </w>', 'w i d e r </w>']\n",
            "Most frequent pair: ('new', '</w>')\n",
            "Updated corpus: ['low</w>', 'lowest</w>', 'new</w>', 'w i d e r </w>']\n",
            "Most frequent pair: ('w', 'i')\n",
            "Updated corpus: ['low</w>', 'lowest</w>', 'new</w>', 'wi d e r </w>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6: Visualizing the Final Vocabulary\n",
        "After applying BPE, the corpus will consist of subwords that are frequently occurring patterns. These subwords form the final vocabulary."
      ],
      "metadata": {
        "id": "Hya7zb0K3saY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract final vocabulary\n",
        "def extract_vocabulary(corpus):\n",
        "    vocab = set()\n",
        "    for word in corpus:\n",
        "        vocab.update(word.split())\n",
        "    return vocab\n",
        "\n",
        "# Display the final vocabulary\n",
        "final_vocabulary = extract_vocabulary(final_corpus)\n",
        "print(\"Final Vocabulary:\", final_vocabulary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avmfwejH3v-z",
        "outputId": "2ae0c539-2022-44bc-a9c6-62a5ea8639d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Vocabulary: {'r', 'd', 'lowest</w>', 'low</w>', 'wi', 'e', 'new</w>', '</w>'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion\n",
        "This notebook demonstrates how Byte Pair Encoding (BPE) works step by step. By iteratively merging the most frequent token pairs, BPE generates a compact vocabulary of subwords, which is widely used in NLP for efficient tokenization."
      ],
      "metadata": {
        "id": "ZENJefwQ3zfQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Appendix - 1: The End-of-Word Marker (`</w>`)**\n",
        "\n",
        "In Byte Pair Encoding (BPE), the `</w>` marker is added to indicate the end of a word. This ensures that subwords appearing at the end of a word are treated differently from those in the middle of another word. For example:\n",
        "- The word `low` becomes `l o w </w>` to indicate it's a complete word.\n",
        "- This prevents confusion between `low` as a standalone word and `low` as part of another word, like `lowest`.\n",
        "\n",
        "Here’s how to tokenize the corpus with the `</w>` marker:\n"
      ],
      "metadata": {
        "id": "gYEIbZ_X3_-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to tokenize each word into characters with an end-of-word marker\n",
        "def tokenize_corpus(corpus):\n",
        "    return [\" \".join(word) + \" </w>\" for word in corpus]  # Add </w> to mark the end of the word\n",
        "\n",
        "# Example: Tokenizing with the end-of-word marker\n",
        "corpus = [\"low\", \"lowest\", \"new\", \"wider\"]\n",
        "tokenized_corpus = tokenize_corpus(corpus)\n",
        "\n",
        "# Display the tokenized corpus with end-of-word markers\n",
        "print(\"Tokenized Corpus with </w> marker:\", tokenized_corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWBik0kL4Rew",
        "outputId": "400d8e75-e00f-452c-b900-e53ea8746711"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized Corpus with </w> marker: ['l o w </w>', 'l o w e s t </w>', 'n e w </w>', 'w i d e r </w>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Appendix - 2: Byte Pair Encoding with `tiktoken` Library**\n",
        "\n",
        "In this section, we'll use the `tiktoken` library to perform BPE tokenization. `tiktoken` is an open-source tokenizer library developed by OpenAI, commonly used with GPT models. It provides efficient implementations of BPE and other tokenization algorithms.\n",
        "\n",
        "#### **Installation**\n",
        "\n",
        "First, we need to install the `tiktoken` library:\n"
      ],
      "metadata": {
        "id": "-hjaoapn4hWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install tiktoken library\n",
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmYq7loB4uds",
        "outputId": "e99e68df-a617-4f6b-ba77-422ac959e6d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Using tiktoken for BPE Tokenization\n",
        "The tiktoken library comes with pre-trained BPE encoders. For demonstration purposes, we'll use the cl100k_base encoder, which is used by models like gpt-3.5-turbo."
      ],
      "metadata": {
        "id": "n1S0VaVu4zHo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "# Load the BPE encoder\n",
        "encoding = tiktoken.get_encoding(\"cl100k_base\")"
      ],
      "metadata": {
        "id": "PjrhovyN45Wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokenizing Text with BPE\n",
        "Let's tokenize some sample text using the BPE encoder."
      ],
      "metadata": {
        "id": "tAuxLRDW5A7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text\n",
        "text = \"This is an example of Byte Pair Encoding using tiktoken library.\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = encoding.encode(text)\n",
        "\n",
        "# Display the tokens\n",
        "print(\"Tokens:\", tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rnK7JiY40tm",
        "outputId": "c156956f-1aa8-472b-ec67-18d75b7b333f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: [10516, 15821, 502, 22622]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Decoding Tokens Back to Text\n",
        "We can also decode the tokens back to the original text to verify the correctness."
      ],
      "metadata": {
        "id": "jii_49Ib5G1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode the tokens back to text\n",
        "decoded_text = encoding.decode(tokens)\n",
        "\n",
        "# Display the decoded text\n",
        "print(\"Decoded Text:\", decoded_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPHS2EtW5Kzy",
        "outputId": "2eaa6d73-a751-4939-d00c-686b2ea9d503"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoded Text: low lowest new wider\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inspecting the Tokens and Their Corresponding Subwords\n",
        "Let's see what subwords each token corresponds to."
      ],
      "metadata": {
        "id": "1Z6PoseX5N1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display tokens with their corresponding subwords\n",
        "token_subwords = [encoding.decode([token]) for token in tokens]\n",
        "print(\"Token - Subword Mapping:\")\n",
        "for token, subword in zip(tokens, token_subwords):\n",
        "    print(f\"{token}: '{subword}'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8u9N7l5s5QfQ",
        "outputId": "6d795e30-45f0-43a1-b4ef-217a354dc1a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token - Subword Mapping:\n",
            "10516: 'low'\n",
            "15821: ' lowest'\n",
            "502: ' new'\n",
            "22622: ' wider'\n"
          ]
        }
      ]
    }
  ]
}