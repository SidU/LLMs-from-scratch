{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMhnpW+pyPQTLUmbbNMWhKC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SidU/LLMs-from-scratch/blob/main/Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Continuous Bag of Words (CBOW) Example Using PyTorch\n",
        "\n",
        "This notebook demonstrates how CBOW works using PyTorch. It includes:\n",
        "1. Vocabulary and embedding matrix setup.\n",
        "2. Retrieving embeddings for context words.\n",
        "3. Averaging context embeddings.\n",
        "4. Predicting the target word and updating embeddings."
      ],
      "metadata": {
        "id": "83TC61IQDmx1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import libraries\n"
      ],
      "metadata": {
        "id": "6Rs9w3_oDp-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "Albr892LDt03"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vocabulary and Embedding Matrix\n",
        "We'll start by defining a small vocabulary of 4 words \"cat\", \"dog\", \"mouse\", and \"cheese\" and an embedding size of 3. An embedding layer will store random embeddings for each word initially."
      ],
      "metadata": {
        "id": "Ld7UYF6ZDzVw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define vocabulary and embedding dimensions\n",
        "vocab = [\"cat\", \"dog\", \"mouse\", \"cheese\"]\n",
        "vocab_size = len(vocab)  # Vocabulary size\n",
        "embedding_dim = 3  # Embedding dimensions\n",
        "\n",
        "# Embedding layer\n",
        "embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "# Initialize the embedding matrix randomly\n",
        "torch.manual_seed(42)  # For reproducibility\n",
        "print(\"Initial Embedding Matrix:\")\n",
        "print(embedding_layer.weight.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFYPzdZZD1yi",
        "outputId": "ee855881-34d3-42d0-b3b7-7eec1f29fcbd"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Embedding Matrix:\n",
            "tensor([[ 0.4617,  0.2674,  0.5349],\n",
            "        [ 0.8094,  1.1103, -1.6898],\n",
            "        [-0.9890,  0.9580,  1.3221],\n",
            "        [ 0.8172, -0.7658, -0.7506]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Context Words and Target Word\n",
        "We define two context words (`\"dog\"` and `\"mouse\"`) and a target word (`\"cat\"`).\n",
        "Context words help predict the target word."
      ],
      "metadata": {
        "id": "3lywXol5EEIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define context and target words\n",
        "context_words = [\"dog\", \"mouse\"]  # Context words\n",
        "target_word = \"cat\"  # Target word\n",
        "\n",
        "# Map words to indices\n",
        "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
        "\n",
        "# Convert context words to indices\n",
        "context_indices = torch.tensor([word_to_idx[word] for word in context_words])  # [1, 2]\n",
        "print(\"Context Indices:\", context_indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kc1lfY9OEJl-",
        "outputId": "f2760a19-e8c3-435b-f464-00d9944e7e94"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context Indices: tensor([1, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieve Embeddings for Context Words\n",
        "Using the indices of the context words, we retrieve their embeddings from the embedding matrix."
      ],
      "metadata": {
        "id": "ZAJFTDuzEVVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Look up embeddings for context words\n",
        "context_embeddings = embedding_layer(context_indices)\n",
        "print(\"Context Embeddings:\")\n",
        "print(context_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHY8AoABEXYk",
        "outputId": "a107104f-9867-4200-bb75-23d86fd79318"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context Embeddings:\n",
            "tensor([[ 0.8094,  1.1103, -1.6898],\n",
            "        [-0.9890,  0.9580,  1.3221]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Averaging Context Embeddings\n",
        "We average the embeddings for context words to form the **context vector**,\n",
        "which represents the context of the sentence."
      ],
      "metadata": {
        "id": "M7MoVpC4EdSN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Average the embeddings to get the context vector\n",
        "context_vector = context_embeddings.mean(dim=0) # dim=0 means average along the rows\n",
        "print(\"Averaged Context Vector:\")\n",
        "print(context_vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2WOI7DcEfYR",
        "outputId": "29cc5f5c-da61-412c-f240-f8b12660f8cc"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Averaged Context Vector:\n",
            "tensor([-0.0898,  1.0341, -0.1838], grad_fn=<MeanBackward1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict the Target Word\n",
        "The averaged context vector is passed through a linear layer to predict the target word.\n",
        "The output is converted into probabilities using the softmax function."
      ],
      "metadata": {
        "id": "DhEN9skyE_Nk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Output layer to map context vector to vocabulary\n",
        "output_layer = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "# Predict target word\n",
        "output_logits = output_layer(context_vector)\n",
        "output_probs = nn.Softmax(dim=-1)(output_logits)\n",
        "print(\"Predicted Probabilities for Vocabulary Words:\")\n",
        "print({idx_to_word[i]: p for i, p in enumerate(output_probs.tolist())})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fwh-D0siFBpT",
        "outputId": "b8219e76-ad3c-4a1c-9b9e-25cbcf6804a1"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Probabilities for Vocabulary Words:\n",
            "{'cat': 0.3702388107776642, 'dog': 0.1323089450597763, 'mouse': 0.26178431510925293, 'cheese': 0.23566798865795135}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss Computation and Backpropagation\n",
        "We compute the cross-entropy loss between the predicted probabilities and the true target word.\n",
        "The embedding matrix and linear layer are updated using backpropagation."
      ],
      "metadata": {
        "id": "NE2wTjO4FXnP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define target index\n",
        "target_index = torch.tensor([word_to_idx[target_word]])  # Index of \"cat\"\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(list(embedding_layer.parameters()) + list(output_layer.parameters()), lr=0.1)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10):  # Number of epochs\n",
        "    optimizer.zero_grad()  # Clear previous gradients\n",
        "\n",
        "    # Recompute context embeddings for the current parameters\n",
        "    context_embeddings = embedding_layer(context_indices)  # Get embeddings for context words\n",
        "    context_vector = context_embeddings.mean(dim=0)  # Average context embeddings\n",
        "\n",
        "    # Forward pass\n",
        "    output_logits = output_layer(context_vector)\n",
        "    loss = criterion(output_logits.unsqueeze(0), target_index)  # Compute loss\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "    optimizer.step()  # Update parameters\n",
        "\n",
        "    # Print loss for each epoch\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
        "\n",
        "# Check updated embedding matrix\n",
        "print(\"Updated Embedding Matrix:\")\n",
        "print(embedding_layer.weight.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysbz5M0cFZWO",
        "outputId": "5f05a1aa-39c0-47e7-c463-fe20b0c48a2f"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.9936070442199707\n",
            "Epoch 2, Loss: 0.8731562495231628\n",
            "Epoch 3, Loss: 0.7691994309425354\n",
            "Epoch 4, Loss: 0.6796515583992004\n",
            "Epoch 5, Loss: 0.602611780166626\n",
            "Epoch 6, Loss: 0.5363603234291077\n",
            "Epoch 7, Loss: 0.479358434677124\n",
            "Epoch 8, Loss: 0.4302491545677185\n",
            "Epoch 9, Loss: 0.38785070180892944\n",
            "Epoch 10, Loss: 0.35114580392837524\n",
            "Updated Embedding Matrix:\n",
            "tensor([[ 0.4617,  0.2674,  0.5349],\n",
            "        [ 0.9373,  1.2150, -1.7927],\n",
            "        [-0.8611,  1.0626,  1.2192],\n",
            "        [ 0.8172, -0.7658, -0.7506]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Phase: Verify the Model’s Learning\n",
        "\n",
        "After training, we test the model to ensure it has learned the relationship between the context words and the target word.\n",
        "\n",
        "In this test, we use the trained embeddings and output layer to:\n",
        "\n",
        "1.\tRecompute the context vector from the test context words (\"dog\" and \"mouse\").\n",
        "2.\tPredict the target word (\"cat\") using the trained model.\n",
        "3.\tCompare the predicted word with the actual target word to check if the model learned correctly.\n",
        "\n",
        "The test phase does not update the model weights, and no gradients are computed during this process."
      ],
      "metadata": {
        "id": "8J2eAGmZGqbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test phase\n",
        "embedding_layer.eval()  # Set embedding layer to evaluation mode\n",
        "output_layer.eval()  # Set output layer to evaluation mode\n",
        "\n",
        "# Test context words and target\n",
        "test_context_words = [\"dog\", \"mouse\"]\n",
        "test_context_indices = torch.tensor([word_to_idx[word] for word in test_context_words])  # Indices for \"dog\" and \"mouse\"\n",
        "\n",
        "# Recompute context vector for test context\n",
        "with torch.no_grad():  # Disable gradient computations during testing\n",
        "    test_context_embeddings = embedding_layer(test_context_indices)  # Get embeddings for test context\n",
        "    test_context_vector = test_context_embeddings.mean(dim=0)  # Average embeddings\n",
        "\n",
        "    # Predict the target word\n",
        "    test_logits = output_layer(test_context_vector)  # Forward pass\n",
        "    test_probs = nn.Softmax(dim=-1)(test_logits)  # Compute probabilities\n",
        "    predicted_index = torch.argmax(test_probs).item()  # Get the index of the highest probability word\n",
        "    predicted_word = idx_to_word[predicted_index]  # Map index to word\n",
        "\n",
        "# Print the test result\n",
        "print(f\"Context Words: {test_context_words}\")\n",
        "print(f\"Predicted Word: {predicted_word}\")\n",
        "print(f\"Actual Target Word: {target_word}\")\n",
        "\n",
        "# Check if the prediction matches the target word\n",
        "if predicted_word == target_word:\n",
        "    print(\"The model correctly predicted the target word!\")\n",
        "else:\n",
        "    print(\"The model failed to predict the target word.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwuOC44uGu19",
        "outputId": "cd86964c-af26-4600-876b-f8ca7a4b2d95"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context Words: ['dog', 'mouse']\n",
            "Predicted Word: cat\n",
            "Actual Target Word: cat\n",
            "The model correctly predicted the target word!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Appendix\n",
        "\n",
        "### **Cross-Entropy Loss**\n",
        "In our neural network, the **cross-entropy loss** measures how well the model's predictions match the true labels.\n",
        "\n",
        "For example:\n",
        "- The true label is \"cat,\" represented as a one-hot vector `[1, 0, 0]`.\n",
        "- The model predicts probabilities `[0.7, 0.2, 0.1]` for \"cat,\" \"dog,\" and \"mouse.\"\n",
        "\n",
        "The loss is calculated using:\n",
        "${Loss}$ = - $\\sum_{i=1}^{C} y_i log(\\hat{y}_i)$\n",
        "\n",
        "- $C$: Number of classes.\n",
        "- $y_i$: True label for class $i$ (1 for the correct class, 0 otherwise).\n",
        "- $\\hat{y}_i$: Predicted probability for class $i$.\n",
        "\n",
        "For this example:\n",
        "${Loss} = - (\\log(0.7)) \\approx 0.356\n",
        "$]\n",
        "\n",
        "A smaller loss means the model is predicting more accurately."
      ],
      "metadata": {
        "id": "HrCBg0frRWfU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-Entropy Loss Example\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# True label (one-hot encoded)\n",
        "true_label = torch.tensor([1, 0, 0])  # \"cat\"\n",
        "# Predicted probabilities (from the model's softmax output)\n",
        "predicted_probs = torch.tensor([0.7, 0.2, 0.1], requires_grad=True)\n",
        "\n",
        "# Calculate cross-entropy loss manually\n",
        "loss = -torch.sum(true_label * torch.log(predicted_probs))\n",
        "print(f\"Cross-Entropy Loss (manual calculation): {loss.item():.4f}\")\n",
        "\n",
        "# Using PyTorch's built-in cross-entropy loss\n",
        "# True label as a class index and predicted logits\n",
        "true_label_index = torch.tensor([0])  # \"cat\" corresponds to index 0\n",
        "logits = torch.tensor([[2.0, 1.0, 0.1]], requires_grad=True)  # Example raw logits\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "loss_builtin = criterion(logits, true_label_index)\n",
        "print(f\"Cross-Entropy Loss (PyTorch): {loss_builtin.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riqwHGiDSk10",
        "outputId": "b8549325-bd3f-4ba4-9cce-1817a7f148e8"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-Entropy Loss (manual calculation): 0.3567\n",
            "Cross-Entropy Loss (PyTorch): 0.4170\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Stochastic Gradient Descent (SGD)**\n",
        "**SGD** is the algorithm used to minimize the loss by updating the model's weights. Here's how it works:\n",
        "1. **Calculate the Gradient**: The gradient tells us how the weights need to change to reduce the loss.\n",
        "2. **Update the Weights**:\n",
        "\n",
        "$$w_{\\text{new}} = w_{\\text{old}} - \\eta \\cdot \\frac{\\partial \\text{Loss}}{\\partial w}$$\n",
        "\n",
        "- $w$: Weight.\n",
        "- $\\eta$: Learning rate (controls step size).\n",
        "\n",
        "In our example, after calculating the loss, we use SGD to adjust the model's weights to better predict the next time.\n"
      ],
      "metadata": {
        "id": "v8YpVH7ESo36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# Initialize logits (weights to optimize)\n",
        "manual_logits = torch.tensor([[2.0, -1.0]], requires_grad=True)  # For manual SGD\n",
        "pytorch_logits = torch.tensor([[2.0, -1.0]], requires_grad=True)  # For PyTorch SGD\n",
        "\n",
        "# True label (class index)\n",
        "true_label = torch.tensor([0])  # Class 0 is the correct class\n",
        "\n",
        "# Learning rate\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Define PyTorch optimizer\n",
        "optimizer = optim.SGD([pytorch_logits], lr=learning_rate)\n",
        "\n",
        "print(f\"Starting Logits: Manual = {manual_logits}, PyTorch = {pytorch_logits}\\n\")\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10):\n",
        "    # ---- Manual SGD ----\n",
        "    # Compute Cross-Entropy Loss (manually)\n",
        "    manual_loss = F.cross_entropy(manual_logits, true_label)  # Takes raw logits and class index\n",
        "\n",
        "    # Backpropagation\n",
        "    manual_loss.backward()\n",
        "\n",
        "    # Update logits using manual SGD\n",
        "    with torch.no_grad():\n",
        "        manual_logits -= learning_rate * manual_logits.grad  # Gradient descent step\n",
        "        manual_logits.grad.zero_()  # Clear gradients\n",
        "\n",
        "    # ---- PyTorch SGD ----\n",
        "    # Compute Cross-Entropy Loss\n",
        "    pytorch_loss = F.cross_entropy(pytorch_logits, true_label)\n",
        "\n",
        "    # Backpropagation\n",
        "    optimizer.zero_grad()\n",
        "    pytorch_loss.backward()\n",
        "    optimizer.step()  # PyTorch's built-in SGD\n",
        "\n",
        "    # Print progress\n",
        "    print(f\"Epoch {epoch+1}\")\n",
        "    print(f\"  Manual: Loss = {manual_loss.item():.4f}, Logits = {manual_logits}\")\n",
        "    print(f\"  PyTorch: Loss = {pytorch_loss.item():.4f}, Logits = {pytorch_logits}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPkG9vhLTR1C",
        "outputId": "18f869d2-3df7-487b-fd45-233e00828493"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Logits: Manual = tensor([[ 2., -1.]], requires_grad=True), PyTorch = tensor([[ 2., -1.]], requires_grad=True)\n",
            "\n",
            "Epoch 1\n",
            "  Manual: Loss = 0.0486, Logits = tensor([[ 2.0047, -1.0047]], requires_grad=True)\n",
            "  PyTorch: Loss = 0.0486, Logits = tensor([[ 2.0047, -1.0047]], requires_grad=True)\n",
            "\n",
            "Epoch 2\n",
            "  Manual: Loss = 0.0481, Logits = tensor([[ 2.0094, -1.0094]], requires_grad=True)\n",
            "  PyTorch: Loss = 0.0481, Logits = tensor([[ 2.0094, -1.0094]], requires_grad=True)\n",
            "\n",
            "Epoch 3\n",
            "  Manual: Loss = 0.0477, Logits = tensor([[ 2.0141, -1.0141]], requires_grad=True)\n",
            "  PyTorch: Loss = 0.0477, Logits = tensor([[ 2.0141, -1.0141]], requires_grad=True)\n",
            "\n",
            "Epoch 4\n",
            "  Manual: Loss = 0.0473, Logits = tensor([[ 2.0187, -1.0187]], requires_grad=True)\n",
            "  PyTorch: Loss = 0.0473, Logits = tensor([[ 2.0187, -1.0187]], requires_grad=True)\n",
            "\n",
            "Epoch 5\n",
            "  Manual: Loss = 0.0468, Logits = tensor([[ 2.0233, -1.0233]], requires_grad=True)\n",
            "  PyTorch: Loss = 0.0468, Logits = tensor([[ 2.0233, -1.0233]], requires_grad=True)\n",
            "\n",
            "Epoch 6\n",
            "  Manual: Loss = 0.0464, Logits = tensor([[ 2.0278, -1.0278]], requires_grad=True)\n",
            "  PyTorch: Loss = 0.0464, Logits = tensor([[ 2.0278, -1.0278]], requires_grad=True)\n",
            "\n",
            "Epoch 7\n",
            "  Manual: Loss = 0.0460, Logits = tensor([[ 2.0323, -1.0323]], requires_grad=True)\n",
            "  PyTorch: Loss = 0.0460, Logits = tensor([[ 2.0323, -1.0323]], requires_grad=True)\n",
            "\n",
            "Epoch 8\n",
            "  Manual: Loss = 0.0456, Logits = tensor([[ 2.0368, -1.0368]], requires_grad=True)\n",
            "  PyTorch: Loss = 0.0456, Logits = tensor([[ 2.0368, -1.0368]], requires_grad=True)\n",
            "\n",
            "Epoch 9\n",
            "  Manual: Loss = 0.0452, Logits = tensor([[ 2.0412, -1.0412]], requires_grad=True)\n",
            "  PyTorch: Loss = 0.0452, Logits = tensor([[ 2.0412, -1.0412]], requires_grad=True)\n",
            "\n",
            "Epoch 10\n",
            "  Manual: Loss = 0.0448, Logits = tensor([[ 2.0456, -1.0456]], requires_grad=True)\n",
            "  PyTorch: Loss = 0.0448, Logits = tensor([[ 2.0456, -1.0456]], requires_grad=True)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}