{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvFyFr6AuamMs678bAyGLd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SidU/LLMs-from-scratch/blob/main/Embeddings_Intuition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# From Word2Vec to Transformers: Exploring Word Embeddings Through CBOW\n",
        "\n",
        "[Word2Vec](https://arxiv.org/abs/1301.3781) revolutionized NLP by introducing dense vector representations for words, capturing their semantic relationships in a high-dimensional space. Though modern models like GPT and transformers leverage dynamic, context-aware embeddings, Word2Vec remains essential for understanding the foundations of word representations, gradient-based optimization, and techniques like negative sampling. Its simplicity makes it an excellent starting point: for learning how embeddings work and understanding their role in modern NLP.\n",
        "\n",
        "In this notebook, we demonstrate the **Continuous Bag of Words (CBOW)** model using PyTorch. You'll learn how to:\n",
        "1. Set up a vocabulary and initialize an embedding matrix.\n",
        "2. Retrieve and average embeddings for context words.\n",
        "3. Predict the target word based on context.\n",
        "4. Update embeddings during training to improve predictions.\n",
        "\n",
        "This step-by-step walkthrough bridges the gap between classic NLP methods and advanced techniques in transformer-based models like GPT."
      ],
      "metadata": {
        "id": "83TC61IQDmx1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import libraries\n"
      ],
      "metadata": {
        "id": "6Rs9w3_oDp-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "id": "Albr892LDt03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7be2e88-c9bf-487c-f50e-38065c242eed"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5.1+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vocabulary and Embedding Matrix\n",
        "We'll start by defining a small vocabulary of 4 words \"cat\", \"dog\", \"mouse\", and \"cheese\" and an embedding size of 3. An embedding layer will store random embeddings for each word initially."
      ],
      "metadata": {
        "id": "Ld7UYF6ZDzVw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define vocabulary and embedding dimensions\n",
        "vocab = [\"cat\", \"dog\", \"mouse\", \"cheese\"]\n",
        "vocab_size = len(vocab)  # Vocabulary size\n",
        "embedding_dim = 3  # Embedding dimensions\n",
        "\n",
        "# Embedding layer\n",
        "embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "# Initialize the embedding matrix randomly\n",
        "torch.manual_seed(42)  # For reproducibility\n",
        "print(\"Initial Embedding Matrix:\")\n",
        "print(embedding_layer.weight.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFYPzdZZD1yi",
        "outputId": "0bb825e5-476e-4de1-cdf5-2532f5021486"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Embedding Matrix:\n",
            "tensor([[-0.1516,  0.8163, -0.9120],\n",
            "        [ 0.0919,  1.2169, -0.8224],\n",
            "        [ 0.7437,  1.0844, -1.4901],\n",
            "        [-1.0669, -0.0683, -1.3577]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Context Words and Target Word\n",
        "We define two context words (`\"dog\"` and `\"mouse\"`) and a target word (`\"cat\"`).\n",
        "Context words help predict the target word."
      ],
      "metadata": {
        "id": "3lywXol5EEIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define context and target words\n",
        "context_words = [\"dog\", \"mouse\"]  # Context words\n",
        "target_word = \"cat\"  # Target word\n",
        "\n",
        "# Map words to indices\n",
        "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
        "\n",
        "# Convert context words to indices\n",
        "context_indices = torch.tensor([word_to_idx[word] for word in context_words])  # [1, 2]\n",
        "print(\"Context Indices:\", context_indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kc1lfY9OEJl-",
        "outputId": "18313302-78ae-46f4-c72b-442c91615462"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context Indices: tensor([1, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieve Embeddings for Context Words\n",
        "Using the indices of the context words, we retrieve their embeddings from the embedding matrix."
      ],
      "metadata": {
        "id": "ZAJFTDuzEVVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Look up embeddings for context words\n",
        "context_embeddings = embedding_layer(context_indices)\n",
        "print(\"Context Embeddings:\")\n",
        "print(context_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHY8AoABEXYk",
        "outputId": "5cf071a6-3616-4098-ee17-b4487c05ada9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context Embeddings:\n",
            "tensor([[ 0.0919,  1.2169, -0.8224],\n",
            "        [ 0.7437,  1.0844, -1.4901]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Averaging Context Embeddings\n",
        "We average the embeddings for context words to form the **context vector**,\n",
        "which represents the context of the sentence."
      ],
      "metadata": {
        "id": "M7MoVpC4EdSN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Average the embeddings to get the context vector\n",
        "context_vector = context_embeddings.mean(dim=0) # dim=0 means average along the rows\n",
        "print(\"Averaged Context Vector:\")\n",
        "print(context_vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2WOI7DcEfYR",
        "outputId": "b6cac535-a0d5-4509-9372-1b71b6cedaa7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Averaged Context Vector:\n",
            "tensor([ 0.4178,  1.1506, -1.1562], grad_fn=<MeanBackward1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict the Target Word\n",
        "The averaged context vector is passed through a linear layer to predict the target word.\n",
        "The output is converted into probabilities using the softmax function."
      ],
      "metadata": {
        "id": "DhEN9skyE_Nk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Output layer to map context vector to vocabulary\n",
        "output_layer = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "# Predict target word\n",
        "output_logits = output_layer(context_vector)\n",
        "output_probs = nn.Softmax(dim=-1)(output_logits)\n",
        "print(\"Predicted Probabilities for Vocabulary Words:\")\n",
        "print({idx_to_word[i]: p for i, p in enumerate(output_probs.tolist())})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fwh-D0siFBpT",
        "outputId": "72835701-3217-4da7-c03e-6eee8bb96dc1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Probabilities for Vocabulary Words:\n",
            "{'cat': 0.539042055606842, 'dog': 0.1470172107219696, 'mouse': 0.13889023661613464, 'cheese': 0.17505046725273132}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss Computation and Backpropagation\n",
        "We compute the cross-entropy loss between the predicted probabilities and the true target word.\n",
        "The embedding matrix and linear layer are updated using backpropagation."
      ],
      "metadata": {
        "id": "NE2wTjO4FXnP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define target index\n",
        "target_index = torch.tensor([word_to_idx[target_word]])  # Index of \"cat\"\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(list(embedding_layer.parameters()) + list(output_layer.parameters()), lr=0.1)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10):  # Number of epochs\n",
        "    optimizer.zero_grad()  # Clear previous gradients\n",
        "\n",
        "    # Recompute context embeddings for the current parameters\n",
        "    context_embeddings = embedding_layer(context_indices)  # Get embeddings for context words\n",
        "    context_vector = context_embeddings.mean(dim=0)  # Average context embeddings\n",
        "\n",
        "    # Forward pass\n",
        "    output_logits = output_layer(context_vector)\n",
        "    loss = criterion(output_logits.unsqueeze(0), target_index)  # Compute loss\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "    optimizer.step()  # Update parameters\n",
        "\n",
        "    # Print loss for each epoch\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
        "\n",
        "# Check updated embedding matrix\n",
        "print(\"Updated Embedding Matrix:\")\n",
        "print(embedding_layer.weight.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysbz5M0cFZWO",
        "outputId": "a8fda7f0-c287-4216-ba95-42e3f23c17b8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.6179617047309875\n",
            "Epoch 2, Loss: 0.5112006664276123\n",
            "Epoch 3, Loss: 0.42886772751808167\n",
            "Epoch 4, Loss: 0.3648257553577423\n",
            "Epoch 5, Loss: 0.3144245743751526\n",
            "Epoch 6, Loss: 0.27423521876335144\n",
            "Epoch 7, Loss: 0.24175719916820526\n",
            "Epoch 8, Loss: 0.2151678204536438\n",
            "Epoch 9, Loss: 0.1931309700012207\n",
            "Epoch 10, Loss: 0.1746586412191391\n",
            "Updated Embedding Matrix:\n",
            "tensor([[-0.1516,  0.8163, -0.9120],\n",
            "        [ 0.1725,  1.2753, -0.8999],\n",
            "        [ 0.8244,  1.1429, -1.5676],\n",
            "        [-1.0669, -0.0683, -1.3577]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Phase: Verify the Modelâ€™s Learning\n",
        "\n",
        "After training, we test the model to ensure it has learned the relationship between the context words and the target word.\n",
        "\n",
        "In this test, we use the trained embeddings and output layer to:\n",
        "\n",
        "1.\tRecompute the context vector from the test context words (\"dog\" and \"mouse\").\n",
        "2.\tPredict the target word (\"cat\") using the trained model.\n",
        "3.\tCompare the predicted word with the actual target word to check if the model learned correctly.\n",
        "\n",
        "The test phase does not update the model weights, and no gradients are computed during this process."
      ],
      "metadata": {
        "id": "8J2eAGmZGqbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test phase\n",
        "embedding_layer.eval()  # Set embedding layer to evaluation mode\n",
        "output_layer.eval()  # Set output layer to evaluation mode\n",
        "\n",
        "# Test context words and target\n",
        "test_context_words = [\"dog\", \"mouse\"]\n",
        "test_context_indices = torch.tensor([word_to_idx[word] for word in test_context_words])  # Indices for \"dog\" and \"mouse\"\n",
        "\n",
        "# Recompute context vector for test context\n",
        "with torch.no_grad():  # Disable gradient computations during testing\n",
        "    test_context_embeddings = embedding_layer(test_context_indices)  # Get embeddings for test context\n",
        "    test_context_vector = test_context_embeddings.mean(dim=0)  # Average embeddings\n",
        "\n",
        "    # Predict the target word\n",
        "    test_logits = output_layer(test_context_vector)  # Forward pass\n",
        "    test_probs = nn.Softmax(dim=-1)(test_logits)  # Compute probabilities\n",
        "    predicted_index = torch.argmax(test_probs).item()  # Get the index of the highest probability word\n",
        "    predicted_word = idx_to_word[predicted_index]  # Map index to word\n",
        "\n",
        "# Print the test result\n",
        "print(f\"Context Words: {test_context_words}\")\n",
        "print(f\"Predicted Word: {predicted_word}\")\n",
        "print(f\"Actual Target Word: {target_word}\")\n",
        "\n",
        "# Check if the prediction matches the target word\n",
        "if predicted_word == target_word:\n",
        "    print(\"The model correctly predicted the target word!\")\n",
        "else:\n",
        "    print(\"The model failed to predict the target word.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwuOC44uGu19",
        "outputId": "70839340-cde2-4410-fc84-b6c56d655a74"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context Words: ['dog', 'mouse']\n",
            "Predicted Word: cat\n",
            "Actual Target Word: cat\n",
            "The model correctly predicted the target word!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Appendix\n",
        "\n",
        "### **Cross-Entropy Loss**\n",
        "In our neural network, the **cross-entropy loss** measures how well the model's predictions match the true labels.\n",
        "\n",
        "For example:\n",
        "- The true label is \"cat,\" represented as a one-hot vector `[1, 0, 0]`.\n",
        "- The model predicts probabilities `[0.7, 0.2, 0.1]` for \"cat,\" \"dog,\" and \"mouse.\"\n",
        "\n",
        "The loss is calculated using:\n",
        "${Loss}$ = - $\\sum_{i=1}^{C} y_i log(\\hat{y}_i)$\n",
        "\n",
        "- $C$: Number of classes.\n",
        "- $y_i$: True label for class $i$ (1 for the correct class, 0 otherwise).\n",
        "- $\\hat{y}_i$: Predicted probability for class $i$.\n",
        "\n",
        "For this example:\n",
        "${Loss} = - (\\log(0.7)) \\approx 0.356\n",
        "$]\n",
        "\n",
        "A smaller loss means the model is predicting more accurately."
      ],
      "metadata": {
        "id": "HrCBg0frRWfU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# True class index (e.g., \"cat\" corresponds to index 0)\n",
        "true_label_index = torch.tensor([0])  # Class 0 is the true label\n",
        "\n",
        "# Raw logits (model output before applying softmax)\n",
        "logits = torch.tensor([[2.0, 1.0, 0.1]], requires_grad=True)\n",
        "\n",
        "# **Manual Calculation**\n",
        "# Step 1: Apply log-softmax to logits\n",
        "log_softmax = torch.log_softmax(logits, dim=1)  # Log probabilities for each class\n",
        "# Step 2: Extract the log probability of the true class\n",
        "loss_manual = -log_softmax[0, true_label_index]\n",
        "\n",
        "# **Using PyTorch Built-in Cross-Entropy Loss**\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "loss_builtin = criterion(logits, true_label_index)\n",
        "\n",
        "# Print results\n",
        "print(f\"Manual Loss: {loss_manual.item():.4f}\")\n",
        "print(f\"PyTorch Loss: {loss_builtin.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riqwHGiDSk10",
        "outputId": "208b91a2-c90b-4990-d832-49baf2409d53"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Manual Loss: 0.4170\n",
            "PyTorch Loss: 0.4170\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Stochastic Gradient Descent (SGD)**\n",
        "**SGD** is the algorithm used to minimize the loss by updating the model's weights. Here's how it works:\n",
        "1. **Calculate the Gradient**: The gradient tells us how the weights need to change to reduce the loss.\n",
        "2. **Update the Weights**:\n",
        "\n",
        "$$w_{\\text{new}} = w_{\\text{old}} - \\eta \\cdot \\frac{\\partial \\text{Loss}}{\\partial w}$$\n",
        "\n",
        "- $w$: Weight.\n",
        "- $\\eta$: Learning rate (controls step size).\n",
        "\n",
        "In our example, after calculating the loss, we use SGD to adjust the model's weights to better predict the next time.\n"
      ],
      "metadata": {
        "id": "v8YpVH7ESo36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# Initialize logits (weights to optimize)\n",
        "manual_logits = torch.tensor([[2.0, -1.0]], requires_grad=True)  # For manual SGD\n",
        "pytorch_logits = torch.tensor([[2.0, -1.0]], requires_grad=True)  # For PyTorch SGD\n",
        "\n",
        "# True label (class index)\n",
        "true_label = torch.tensor([0])  # Class 0 is the correct class\n",
        "\n",
        "# Learning rate\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Define PyTorch optimizer\n",
        "optimizer = optim.SGD([pytorch_logits], lr=learning_rate)\n",
        "\n",
        "print(f\"Starting Logits: Manual = {manual_logits}, PyTorch = {pytorch_logits}\\n\")\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10):\n",
        "    # ---- Manual SGD ----\n",
        "    # Compute Cross-Entropy Loss (manually)\n",
        "    manual_loss = F.cross_entropy(manual_logits, true_label)  # Takes raw logits and class index\n",
        "\n",
        "    # Backpropagation\n",
        "    manual_loss.backward()\n",
        "\n",
        "    # Update logits using manual SGD\n",
        "    with torch.no_grad():\n",
        "        manual_logits -= learning_rate * manual_logits.grad  # Gradient descent step\n",
        "        manual_logits.grad.zero_()  # Clear gradients\n",
        "\n",
        "    # ---- PyTorch SGD ----\n",
        "    # Compute Cross-Entropy Loss\n",
        "    pytorch_loss = F.cross_entropy(pytorch_logits, true_label)\n",
        "\n",
        "    # Backpropagation\n",
        "    optimizer.zero_grad()\n",
        "    pytorch_loss.backward()\n",
        "    optimizer.step()  # PyTorch's built-in SGD\n",
        "\n",
        "    # Print progress\n",
        "    print(f\"Epoch {epoch+1}\")\n",
        "    print(f\"  Manual: Loss = {manual_loss.item():.4f}, Logits = {manual_logits}\")\n",
        "    print(f\"  PyTorch: Loss = {pytorch_loss.item():.4f}, Logits = {pytorch_logits}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPkG9vhLTR1C",
        "outputId": "18f869d2-3df7-487b-fd45-233e00828493"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Logits: Manual = tensor([[ 2., -1.]], requires_grad=True), PyTorch = tensor([[ 2., -1.]], requires_grad=True)\n",
            "\n",
            "Epoch 1\n",
            "  Manual: Loss = 0.0486, Logits = tensor([[ 2.0047, -1.0047]], requires_grad=True)\n",
            "  PyTorch: Loss = 0.0486, Logits = tensor([[ 2.0047, -1.0047]], requires_grad=True)\n",
            "\n",
            "Epoch 2\n",
            "  Manual: Loss = 0.0481, Logits = tensor([[ 2.0094, -1.0094]], requires_grad=True)\n",
            "  PyTorch: Loss = 0.0481, Logits = tensor([[ 2.0094, -1.0094]], requires_grad=True)\n",
            "\n",
            "Epoch 3\n",
            "  Manual: Loss = 0.0477, Logits = tensor([[ 2.0141, -1.0141]], requires_grad=True)\n",
            "  PyTorch: Loss = 0.0477, Logits = tensor([[ 2.0141, -1.0141]], requires_grad=True)\n",
            "\n",
            "Epoch 4\n",
            "  Manual: Loss = 0.0473, Logits = tensor([[ 2.0187, -1.0187]], requires_grad=True)\n",
            "  PyTorch: Loss = 0.0473, Logits = tensor([[ 2.0187, -1.0187]], requires_grad=True)\n",
            "\n",
            "Epoch 5\n",
            "  Manual: Loss = 0.0468, Logits = tensor([[ 2.0233, -1.0233]], requires_grad=True)\n",
            "  PyTorch: Loss = 0.0468, Logits = tensor([[ 2.0233, -1.0233]], requires_grad=True)\n",
            "\n",
            "Epoch 6\n",
            "  Manual: Loss = 0.0464, Logits = tensor([[ 2.0278, -1.0278]], requires_grad=True)\n",
            "  PyTorch: Loss = 0.0464, Logits = tensor([[ 2.0278, -1.0278]], requires_grad=True)\n",
            "\n",
            "Epoch 7\n",
            "  Manual: Loss = 0.0460, Logits = tensor([[ 2.0323, -1.0323]], requires_grad=True)\n",
            "  PyTorch: Loss = 0.0460, Logits = tensor([[ 2.0323, -1.0323]], requires_grad=True)\n",
            "\n",
            "Epoch 8\n",
            "  Manual: Loss = 0.0456, Logits = tensor([[ 2.0368, -1.0368]], requires_grad=True)\n",
            "  PyTorch: Loss = 0.0456, Logits = tensor([[ 2.0368, -1.0368]], requires_grad=True)\n",
            "\n",
            "Epoch 9\n",
            "  Manual: Loss = 0.0452, Logits = tensor([[ 2.0412, -1.0412]], requires_grad=True)\n",
            "  PyTorch: Loss = 0.0452, Logits = tensor([[ 2.0412, -1.0412]], requires_grad=True)\n",
            "\n",
            "Epoch 10\n",
            "  Manual: Loss = 0.0448, Logits = tensor([[ 2.0456, -1.0456]], requires_grad=True)\n",
            "  PyTorch: Loss = 0.0448, Logits = tensor([[ 2.0456, -1.0456]], requires_grad=True)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}